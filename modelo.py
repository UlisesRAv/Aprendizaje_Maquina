import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras_preprocessing.text import Tokenizer
from sklearn.preprocessing import LabelBinarizer
import unicodedata


# ================= NORMALIZACI√ìN DE TEXTO =======================
def normalizar(texto):
    texto = texto.lower()
    texto = ''.join(
        c for c in unicodedata.normalize('NFD', texto)
        if unicodedata.category(c) != 'Mn'
    )
    return texto

def crear_y_entrenar_modelo(datos):
    """
    Entrena la red neuronal con los datos proporcionados.
    Ajustado para manejar un vocabulario m√°s amplio.
    """
    
    # 1. PREPROCESAMIENTO
    print("Preprocesando datos...")
    frases = [normalizar(d[0]) for d in datos]
    etiquetas = [d[1] for d in datos]

    # --- AUMENTAR EL VOCABULARIO ---
    tokenizer = Tokenizer(num_words=2600, lower=True) 
    tokenizer.fit_on_texts(frases)
    
    # Convertimos las frases a vectores TF-IDF para capturar la importancia de cada palabra
    X_train = tokenizer.texts_to_matrix(frases, mode='tfidf')

    encoder = LabelBinarizer()
    y_train = encoder.fit_transform(etiquetas)

    # 2. CREACI√ìN DEL MODELO
    print(f"Construyendo red neuronal (Vocabulario: {X_train.shape[1]} palabras, Categor√≠as: {y_train.shape[1]})...")
    model = Sequential()

    # Capa de entrada + Oculta 1
    # Aumentamos de las neuronas (de 16 a 32) para manejar la complejidad de las 25 emociones
    model.add(Dense(32, input_shape=(X_train.shape[1],), activation='tanh'))
    
    # Dropout ayuda a que el modelo no "memorice" demasiado y generalice mejor
    model.add(Dropout(0.3)) 

    # Capa Oculta 2
    model.add(Dense(16, activation='relu'))

    # Capa de salida (softmax para probabilidad de m√∫ltiples clases)
    model.add(Dense(y_train.shape[1], activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    # 3. ENTRENAMIENTO
    print("Entrenando al agente tao√≠sta con sabidur√≠a expandida...")
    # Aumentamos epochs a 250 para asegurar que aprenda bien las diferencias sutiles
    model.fit(X_train, y_train, epochs=250, verbose=0)
    print("Entrenamiento completado.\n")
    
    loss, acc = model.evaluate(X_train, y_train, verbose=0)
    print(f"üìà Precisi√≥n (Accuracy): {acc*100:.2f}%")

    return model, tokenizer, encoder

def obtener_prediccion(texto_usuario, model, tokenizer, encoder, citas_db):
    """
    Predice la emoci√≥n y devuelve la cita correspondiente.
    """
    # Normalizar texto
    texto_usuario = normalizar(texto_usuario)

    # Convertir texto nuevo a n√∫meros usando el MISMO tokenizer entrenado
    vector_texto = tokenizer.texts_to_matrix([texto_usuario], mode='tfidf')
    
    prediccion = model.predict(vector_texto, verbose=0)
    
    # Obtenemos el √≠ndice de la emoci√≥n m√°s probable
    indice_ganador = np.argmax(prediccion)
    
    # Obtenemos la etiqueta de texto (ej: "euforia")
    etiqueta_ganadora = encoder.classes_[indice_ganador]
    
    # Confianza de la predicci√≥n (opcional, √∫til para depurar)
    confianza = prediccion[0][indice_ganador]
    
    # Si la confianza es muy baja (el modelo est√° adivinando), podr√≠amos manejarlo
    # Pero por ahora, confiamos en el Tao.
    
    # Recuperamos la cita (o lista de citas) de la base de datos
    try:
        return citas_db[etiqueta_ganadora]
    except KeyError:
        return ["El Tao es misterioso... no s√© qu√© decirte sobre eso, pero respira."]